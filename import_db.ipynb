{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\"> Import every csv file into their respective table in the database.</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import sqlalchemy as sqla\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_NAME = \"postgres\"\n",
    "DB_USER = \"postgres\"\n",
    "DB_PASS = \"postgres\"\n",
    "DB_HOST = \"localhost\"\n",
    "DB_PORT = \"5432\"\n",
    "\n",
    "SQL_INIT = \"init.sql\"\n",
    "\n",
    "engine = sqla.create_engine(f\"postgresql://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(psycopg2.errors.DuplicateTable) relation \"service\" already exists\n",
      "\n",
      "[SQL: START TRANSACTION;\n",
      "\n",
      "-- For TRAIN_SERV\n",
      "CREATE TABLE SERVICE (\n",
      "    id INT PRIMARY KEY, -- id of the service, autoincrement\n",
      "    name VARCHAR(100) NOT NULL -- name of the service\n",
      ") ;\n",
      "\n",
      "CREATE TABLE REGION(\n",
      "    id INT PRIMARY KEY, -- id of the region, autoincrement\n",
      "    name VARCHAR(255) NOT NULL -- name of the region\n",
      ") ;\n",
      "\n",
      "-- For UNIQUE_STATION\n",
      "CREATE TABLE STATIONS (\n",
      "    id INT PRIMARY KEY, -- id of the station, autoincrement\n",
      "    name VARCHAR(255) NOT NULL, -- name of the station\n",
      "    region INT NOT NULL, -- region of the station\n",
      "    latitude DOUBLE PRECISION NOT NULL, -- latitude of the station\n",
      "    longitude DOUBLE PRECISION NOT NULL, -- longitude of the station\n",
      "\n",
      "    FOREIGN KEY (region) REFERENCES REGION(id)\n",
      ") ;\n",
      "\n",
      "CREATE TABLE RELATION(\n",
      "    id INT PRIMARY KEY, -- id of the relation, autoincrement\n",
      "    name VARCHAR(255) NOT NULL, -- name of the relation\n",
      "    type INT NOT NULL -- type of the relation\n",
      ");\n",
      "\n",
      "\n",
      "CREATE TABLE TRAIN_DATA(\n",
      "    departure_date DATE,\n",
      "    train_number INT,\n",
      "    relation INT,\n",
      "    train_service INT,\n",
      "    ptcar_number INT,\n",
      "    line_number_departure VARCHAR(255),\n",
      "    real_time_arrival TIME,\n",
      "    real_time_departure TIME,\n",
      "    planned_time_arrival TIME,\n",
      "    planned_time_departure TIME,\n",
      "    delay_arrival REAL,\n",
      "    delay_departure REAL,\n",
      "    ptcar_name INT,\n",
      "    line_number_arrival VARCHAR(255),\n",
      "    station_departure INT,\n",
      "    station_arrival INT,\n",
      "\n",
      "    FOREIGN KEY (relation) REFERENCES RELATION(id),\n",
      "    FOREIGN KEY (train_service) REFERENCES SERVICE(id),\n",
      "    FOREIGN KEY (ptcar_name) REFERENCES STATIONS(id),\n",
      "    FOREIGN KEY (station_departure) REFERENCES STATIONS(id),\n",
      "    FOREIGN KEY (station_arrival) REFERENCES STATIONS(id)\n",
      ") ;\n",
      "\n",
      "\n",
      "CREATE TABLE WEATHER (\n",
      "    temperature REAL NOT NULL,\n",
      "    dewpoint REAL NOT NULL, \n",
      "    relative_humidity REAL NOT NULL,\n",
      "    precipitation REAL NOT NULL,\n",
      "    snowfall  REAL NOT NULL,\n",
      "    wind_direction INT NOT NULL,\n",
      "    wind_speed REAL NOT NULL,\n",
      "    pressure REAL NOT NULL,\n",
      "\n",
      "    province INT NOT NULL,\n",
      "    date DATE NOT NULL,\n",
      "    hour TIME NOT NULL,\n",
      "\n",
      "    FOREIGN KEY (province) REFERENCES province(id)\n",
      ") ;\n",
      "\n",
      "CREATE TABLE type_day (\n",
      "    date DATE NOT NULL,\n",
      "    holiday int NOT NULL,\n",
      "    weekend boolean NOT NULL,\n",
      "    day_after_rest boolean NOT NULL\n",
      ") ;\n",
      "\n",
      "CREATE TABLE province(\n",
      "\n",
      "    id INT PRIMARY KEY, -- id of the province, autoincrement\n",
      "    province VARCHAR(255) NOT NULL, -- name of the province\n",
      "    REGION INT NOT NULL, -- region of the province\n",
      "    FOREIGN KEY (REGION) REFERENCES REGION(id)\n",
      ") ;\n",
      "\n",
      "\n",
      "COMMIT;\n",
      "]\n",
      "(Background on this error at: https://sqlalche.me/e/20/f405)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open(SQL_INIT, \"r\") as f:\n",
    "        query = f.read()\n",
    "        with engine.connect() as conn:\n",
    "            conn.execute(sqla.text(query))\n",
    "        print(\"Database initialized\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REGION loaded\n"
     ]
    }
   ],
   "source": [
    "REGION_FILE = \"REGION.csv\"\n",
    "\n",
    "try:\n",
    "    conn = engine.raw_connection()\n",
    "    cursor = conn.cursor()\n",
    "    with(open(REGION_FILE, \"r\")) as f:\n",
    "        cursor.copy_expert(\n",
    "        \"\"\"\n",
    "            COPY region FROM STDIN WITH CSV HEADER DELIMITER ',' NULL ''\n",
    "        \"\"\",f)\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    print(\"REGION loaded\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service loaded\n"
     ]
    }
   ],
   "source": [
    "SERVICE_FILE = \"TRAIN_SERV.csv\"\n",
    "\n",
    "try:\n",
    "    conn = engine.raw_connection()\n",
    "    cursor = conn.cursor()\n",
    "    with(open(SERVICE_FILE, \"r\")) as f:\n",
    "        cursor.copy_expert(\n",
    "        \"\"\"\n",
    "            COPY service FROM STDIN WITH CSV HEADER DELIMITER ',' NULL ''\n",
    "        \"\"\",f)\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    print(\"Service loaded\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stations loaded\n"
     ]
    }
   ],
   "source": [
    "STATIONS_FILE = \"better_station.csv\"\n",
    "try:\n",
    "    conn = engine.raw_connection()\n",
    "    cursor = conn.cursor()\n",
    "    with(open(STATIONS_FILE, \"r\")) as f:\n",
    "        cursor.copy_expert(\n",
    "        \"\"\"\n",
    "            COPY stations FROM STDIN WITH CSV HEADER DELIMITER ',' NULL ''\n",
    "        \"\"\",f)\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    print(\"Stations loaded\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RELATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RELATION loaded\n"
     ]
    }
   ],
   "source": [
    "RELATION_FILE = \"RELATION.csv\"\n",
    "\n",
    "try:\n",
    "    conn = engine.raw_connection()\n",
    "    cursor = conn.cursor()\n",
    "    with(open(RELATION_FILE, \"r\")) as f:\n",
    "        cursor.copy_expert(\n",
    "        \"\"\"\n",
    "            COPY relation FROM STDIN WITH CSV HEADER DELIMITER ',' NULL ''\n",
    "        \"\"\",f)\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    print(\"RELATION loaded\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data_raw_punctuality_201401.csv loaded\n",
      "Data_raw_punctuality_201402.csv loaded\n",
      "Data_raw_punctuality_201407.csv loaded\n",
      "Data_raw_punctuality_201408.csv loaded\n",
      "Data_raw_punctuality_201502.csv loaded\n",
      "Data_raw_punctuality_201506.csv loaded\n",
      "Data_raw_punctuality_201508.csv loaded\n",
      "Data_raw_punctuality_201601.csv loaded\n",
      "Data_raw_punctuality_201606.csv loaded\n",
      "Data_raw_punctuality_201607.csv loaded\n",
      "Data_raw_punctuality_201608.csv loaded\n",
      "Data_raw_punctuality_201612.csv loaded\n",
      "Data_raw_punctuality_201702.csv loaded\n",
      "Data_raw_punctuality_201704.csv loaded\n",
      "Data_raw_punctuality_201708.csv loaded\n",
      "Data_raw_punctuality_201709.csv loaded\n",
      "Data_raw_punctuality_201710.csv loaded\n",
      "Data_raw_punctuality_201802.csv loaded\n",
      "Data_raw_punctuality_201803.csv loaded\n",
      "Data_raw_punctuality_201804.csv loaded\n",
      "Data_raw_punctuality_201805.csv loaded\n",
      "Data_raw_punctuality_201808.csv loaded\n",
      "Data_raw_punctuality_201809.csv loaded\n",
      "Data_raw_punctuality_201810.csv loaded\n",
      "Data_raw_punctuality_201811.csv loaded\n",
      "Data_raw_punctuality_201905.csv loaded\n",
      "Data_raw_punctuality_201907.csv loaded\n",
      "Data_raw_punctuality_202001.csv loaded\n",
      "Data_raw_punctuality_202004.csv loaded\n",
      "Data_raw_punctuality_202009.csv loaded\n",
      "Data_raw_punctuality_202108.csv loaded\n",
      "Data_raw_punctuality_202201.csv loaded\n",
      "Data_raw_punctuality_202204.csv loaded\n",
      "Data_raw_punctuality_202205.csv loaded\n",
      "Data_raw_punctuality_202306.csv loaded\n",
      "Data_raw_punctuality_202307.csv loaded\n",
      "Data_raw_punctuality_202309.csv loaded\n",
      "Data_raw_punctuality_202312.csv loaded\n",
      "Data_raw_punctuality_202401.csv loaded\n",
      "Data_raw_punctuality_202403.csv loaded\n",
      "Data_raw_punctuality_202404.csv loaded\n",
      "Data_raw_punctuality_202409.csv loaded\n",
      "Data_raw_punctuality_202410.csv loaded\n",
      "Data_raw_punctuality_202411.csv loaded\n",
      "Data_raw_punctuality_201403.csv loaded\n",
      "Data_raw_punctuality_201406.csv loaded\n",
      "Data_raw_punctuality_201411.csv loaded\n",
      "Data_raw_punctuality_201503.csv loaded\n",
      "Data_raw_punctuality_201504.csv loaded\n",
      "Data_raw_punctuality_201505.csv loaded\n",
      "Data_raw_punctuality_201509.csv loaded\n",
      "Data_raw_punctuality_201511.csv loaded\n",
      "Data_raw_punctuality_201604.csv loaded\n",
      "Data_raw_punctuality_201605.csv loaded\n",
      "Data_raw_punctuality_201611.csv loaded\n",
      "Data_raw_punctuality_201701.csv loaded\n",
      "Data_raw_punctuality_201705.csv loaded\n",
      "Data_raw_punctuality_201711.csv loaded\n",
      "Data_raw_punctuality_201801.csv loaded\n",
      "Data_raw_punctuality_201806.csv loaded\n",
      "Data_raw_punctuality_201902.csv loaded\n",
      "Data_raw_punctuality_201903.csv loaded\n",
      "Data_raw_punctuality_201908.csv loaded\n",
      "Data_raw_punctuality_201911.csv loaded\n",
      "Data_raw_punctuality_202005.csv loaded\n",
      "Data_raw_punctuality_202006.csv loaded\n",
      "Data_raw_punctuality_202011.csv loaded\n",
      "Data_raw_punctuality_202101.csv loaded\n",
      "Data_raw_punctuality_202102.csv loaded\n",
      "Data_raw_punctuality_202103.csv loaded\n",
      "Data_raw_punctuality_202106.csv loaded\n",
      "Data_raw_punctuality_202109.csv loaded\n",
      "Data_raw_punctuality_202110.csv loaded\n",
      "Data_raw_punctuality_202112.csv loaded\n",
      "Data_raw_punctuality_202207.csv loaded\n",
      "Data_raw_punctuality_202211.csv loaded\n",
      "Data_raw_punctuality_202302.csv loaded\n",
      "Data_raw_punctuality_202303.csv loaded\n",
      "Data_raw_punctuality_202304.csv loaded\n",
      "Data_raw_punctuality_202308.csv loaded\n",
      "Data_raw_punctuality_202310.csv loaded\n",
      "Data_raw_punctuality_202402.csv loaded\n",
      "Data_raw_punctuality_202405.csv loaded\n",
      "Data_raw_punctuality_202407.csv loaded\n",
      "Data_raw_punctuality_202408.csv loaded\n",
      "Data_raw_punctuality_201404.csv loaded\n",
      "Data_raw_punctuality_201405.csv loaded\n",
      "Data_raw_punctuality_201409.csv loaded\n",
      "Data_raw_punctuality_201410.csv loaded\n",
      "Data_raw_punctuality_201412.csv loaded\n",
      "Data_raw_punctuality_201501.csv loaded\n",
      "Data_raw_punctuality_201507.csv loaded\n",
      "Data_raw_punctuality_201510.csv loaded\n",
      "Data_raw_punctuality_201512.csv loaded\n",
      "Data_raw_punctuality_201602.csv loaded\n",
      "Data_raw_punctuality_201603.csv loaded\n",
      "Data_raw_punctuality_201609.csv loaded\n",
      "Data_raw_punctuality_201610.csv loaded\n",
      "Data_raw_punctuality_201703.csv loaded\n",
      "Data_raw_punctuality_201706.csv loaded\n",
      "Data_raw_punctuality_201707.csv loaded\n",
      "Data_raw_punctuality_201712.csv loaded\n",
      "Data_raw_punctuality_201807.csv loaded\n",
      "Data_raw_punctuality_201812.csv loaded\n",
      "Data_raw_punctuality_201901.csv loaded\n",
      "Data_raw_punctuality_201904.csv loaded\n",
      "Data_raw_punctuality_201906.csv loaded\n",
      "Data_raw_punctuality_201909.csv loaded\n",
      "Data_raw_punctuality_201910.csv loaded\n",
      "Data_raw_punctuality_201912.csv loaded\n",
      "Data_raw_punctuality_202002.csv loaded\n",
      "Data_raw_punctuality_202003.csv loaded\n",
      "Data_raw_punctuality_202007.csv loaded\n",
      "Data_raw_punctuality_202008.csv loaded\n",
      "Data_raw_punctuality_202010.csv loaded\n",
      "Data_raw_punctuality_202012.csv loaded\n",
      "Data_raw_punctuality_202104.csv loaded\n",
      "Data_raw_punctuality_202105.csv loaded\n",
      "Data_raw_punctuality_202107.csv loaded\n",
      "Data_raw_punctuality_202111.csv loaded\n",
      "Data_raw_punctuality_202202.csv loaded\n",
      "Data_raw_punctuality_202203.csv loaded\n",
      "Data_raw_punctuality_202206.csv loaded\n",
      "Data_raw_punctuality_202208.csv loaded\n",
      "Data_raw_punctuality_202209.csv loaded\n",
      "Data_raw_punctuality_202210.csv loaded\n",
      "Data_raw_punctuality_202212.csv loaded\n",
      "Data_raw_punctuality_202301.csv loaded\n",
      "Data_raw_punctuality_202305.csv loaded\n",
      "Data_raw_punctuality_202311.csv loaded\n",
      "Data_raw_punctuality_202406.csv loaded\n",
      "Data_raw_punctuality_202412.csv loaded\n"
     ]
    }
   ],
   "source": [
    "# We now load the datasets into the database\n",
    "DATASET_DIR = \"cleaned_dataset\"\n",
    "DATASET_FILES = os.listdir(DATASET_DIR)\n",
    "\n",
    "for file in DATASET_FILES:\n",
    "    try:\n",
    "        conn = engine.raw_connection()\n",
    "        cursor = conn.cursor()\n",
    "        with(open(f\"{DATASET_DIR}/{file}\", \"r\")) as f:\n",
    "            cursor.copy_expert(\n",
    "            \"\"\"\n",
    "                COPY train_data FROM STDIN WITH CSV HEADER DELIMITER ',' NULL ''\n",
    "            \"\"\",f)\n",
    "        conn.commit()\n",
    "        cursor.close()\n",
    "        print(f\"{file} loaded\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file}: {e}\")\n",
    "        exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type_day loaded\n"
     ]
    }
   ],
   "source": [
    "DATE_FILE = \"dates_2014_2024.csv\"\n",
    "\n",
    "try:\n",
    "    conn = engine.raw_connection()\n",
    "    cursor = conn.cursor()\n",
    "    with(open(DATE_FILE, \"r\")) as f:\n",
    "        cursor.copy_expert(\n",
    "        \"\"\"\n",
    "            COPY type_day FROM STDIN WITH CSV HEADER DELIMITER ',' NULL ''\n",
    "        \"\"\",f)\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    print(\"type_day loaded\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROVINCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "province loaded\n"
     ]
    }
   ],
   "source": [
    "DATE_FILE = \"province_mapping.csv\"\n",
    "\n",
    "try:\n",
    "    conn = engine.raw_connection()\n",
    "    cursor = conn.cursor()\n",
    "    with(open(DATE_FILE, \"r\")) as f:\n",
    "        cursor.copy_expert(\n",
    "        \"\"\"\n",
    "            COPY province FROM STDIN WITH CSV HEADER DELIMITER ',' NULL ''\n",
    "        \"\"\",f)\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    print(\"province loaded\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WEATHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weather loaded\n"
     ]
    }
   ],
   "source": [
    "WEATHER_FILE = \"Belgium\\hourly_weather_data.csv\"\n",
    "\n",
    "try:\n",
    "    conn = engine.raw_connection()\n",
    "    cursor = conn.cursor()\n",
    "    with(open(WEATHER_FILE, \"r\")) as f:\n",
    "        cursor.copy_expert(\n",
    "        \"\"\"\n",
    "            COPY weather FROM STDIN WITH CSV HEADER DELIMITER ',' NULL ''\n",
    "        \"\"\",f)\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    print(\"weather loaded\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
