{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "75474b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to PostgreSQL successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sqlalchemy as sqla\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DB_USER = \"postgres\"\n",
    "DB_PASSWORD = \"a\"\n",
    "DB_HOST = \"localhost\"\n",
    "DB_PORT = \"5432\"\n",
    "DB_NAME = \"postgres\"\n",
    "\n",
    "connection_string = f\"postgresql+psycopg2://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "\n",
    "# Créer un moteur SQLAlchemy\n",
    "engine = sqla.create_engine(connection_string)\n",
    "\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        print(\"Connected to PostgreSQL successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "connection = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "651e1718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    id  province_id\n",
      "0  159            0\n",
      "1  151            0\n",
      "2  149            0\n",
      "3  150            0\n",
      "4  156            0\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT * FROM stations;\n",
    "\"\"\"\n",
    "\n",
    "stations = pd.read_sql(sqla.text(query), connection)\n",
    "\n",
    "data_province = [\n",
    "    (0,\"Bruxelles\", 4.367414882050033, 50.83642580516115),\n",
    "    (1,\"Antwerpen\", 4.721043779498864, 51.231900712244745),\n",
    "    (2,\"Limburg\", 5.4357209224443475, 50.98831159244354),\n",
    "    (3,\"Oost-Vlaanderen\", 3.8188605810442007, 51.03629347952689),\n",
    "    (4,\"Vlaams Brabant\", 4.59072072950776, 50.87303943005528),\n",
    "    (5,\"West-Vlaanderen\", 3.0620446668608117, 51.01019740435232),\n",
    "    (6,\"Brabant Wallon\", 4.589736973914637, 50.66605965845173),\n",
    "    (7,\"Hainaut\", 3.9681364929631004, 50.46387637972707),\n",
    "    (8,\"Liège\", 5.7372263688290746, 50.518876085615986),\n",
    "    (9,\"Luxembourg\", 5.516735682549194, 49.95849015160492),\n",
    "    (10,\"Namur\", 4.862346452029854, 50.252686235186815),\n",
    "]\n",
    "\n",
    "province = pd.DataFrame(data_province, columns=[\"province_id\", \"province_name\", \"province_long\", \"province_lat\"])\n",
    "\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    R = 6371  \n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    return R * c\n",
    "\n",
    "def find_nearest_province(station_row):\n",
    "    distances = haversine(\n",
    "        station_row['longitude'], station_row['latitude'],\n",
    "        province['province_long'].values, province['province_lat'].values\n",
    "    )\n",
    "    nearest_idx = distances.argmin()\n",
    "    return province.iloc[nearest_idx]\n",
    "\n",
    "# Apply the function\n",
    "nearest_provinces = stations.apply(find_nearest_province, axis=1)\n",
    "\n",
    "# Concatenate station info with nearest province\n",
    "stations_with_province = pd.concat([stations.reset_index(drop=True), nearest_provinces.reset_index(drop=True)], axis=1)\n",
    "\n",
    "road = [159,151,149,150,156,157,154,245,773,132,310,365,422,300,628,614,591,\n",
    " 540,118,178,235,281,480,108,642,625,623,557,558,384,199,559,645,123,195,73,\n",
    " 261,560,187,438,172,342,341,80,503,629,267,313,608,339,452,564,493,\n",
    " 317,704,757,70]\n",
    "\n",
    "filtered_stations = stations_with_province[stations_with_province['id'].isin(road)]\n",
    "filtered_stations = filtered_stations.drop(columns=['province_long', 'province_lat','region', 'latitude', 'longitude','geom', 'name', 'province_name'])\n",
    "filtered_stations['id'] = pd.Categorical(filtered_stations['id'], categories=road, ordered=True)\n",
    "filtered_stations = filtered_stations.sort_values('id').reset_index(drop=True)\n",
    "\n",
    "print(filtered_stations.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4af26fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Road: 57\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    td.id,\n",
    "    td.departure_date,\n",
    "    td.train_number,\n",
    "    td.relation,\n",
    "    td.train_service,\n",
    "    td.ptcar_number,\n",
    "    td.line_number_departure,\n",
    "    td.real_time_arrival,\n",
    "    td.real_time_departure,\n",
    "    td.planned_time_arrival,\n",
    "    td.planned_time_departure,\n",
    "    td.delay_arrival,\n",
    "    td.delay_departure,\n",
    "    ptcar.id AS ptcar_id,     \n",
    "    ptcar.name AS name_travel,    \n",
    "    td.line_number_arrival,\n",
    "    dep.name AS departure_station_name,\n",
    "    arr.name AS arrival_station_name,\n",
    "    ptcar.longitude AS ptcar_longitude,  \n",
    "    ptcar.latitude AS ptcar_latitude  \n",
    "FROM train_data td\n",
    "JOIN STATIONS ptcar ON td.ptcar_name = ptcar.id\n",
    "JOIN STATIONS dep ON td.station_departure = dep.id\n",
    "JOIN STATIONS arr ON td.station_arrival = arr.id\n",
    "WHERE td.station_arrival = '487'  \n",
    "  AND td.station_departure = '159'  \n",
    "ORDER BY td.id;\n",
    "\"\"\"\n",
    "\n",
    "line = pd.read_sql(sqla.text(query), connection)\n",
    "line =line.drop(columns=['train_service', 'relation'])\n",
    "\n",
    "road = [159,151,149,150,156,157,154,245,773,132,310,365,422,300,628,614,591,\n",
    " 540,118,178,235,281,480,108,642,625,623,557,558,384,199,559,645,123,195,73,\n",
    " 261,560,187,438,172,342,341,80,503,629,267,313,608,339,452,564,493,\n",
    " 317,704,757,70]\n",
    "\n",
    "print(\"Road:\", len(road))\n",
    "\n",
    "tempo = []\n",
    "clean = []\n",
    "road_index = 0\n",
    "\n",
    "for i, row in line.iterrows():\n",
    "    ptcar_id = row['ptcar_id']\n",
    "    if ptcar_id == road[road_index]:\n",
    "        tempo.append(row)\n",
    "        road_index += 1\n",
    "        if len(tempo) == len(road):\n",
    "            clean.extend(tempo)\n",
    "            tempo = []\n",
    "            road_index = 0\n",
    "    else:\n",
    "        tempo = []\n",
    "        road_index = 0\n",
    "\n",
    "clean_df = pd.DataFrame(clean)\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT * FROM TYPE_DAY;\n",
    "\"\"\"\n",
    "\n",
    "type_day = pd.read_sql(sqla.text(query), connection)\n",
    "clean_df= clean_df.drop(columns=['departure_station_name','arrival_station_name','id', 'name_travel', 'ptcar_number','ptcar_id'])\n",
    "merged_df = pd.merge(clean_df, type_day, left_on='departure_date', right_on='date', how='left')\n",
    "merged_df.loc[merged_df.index[::57], \"delay_arrival\"] = 0\n",
    "\n",
    "merged_df.loc[merged_df.index[::57], \"real_time_arrival\"] = merged_df.loc[merged_df.index[::57], \"real_time_departure\"]\n",
    "merged_df.loc[merged_df.index[::57], \"planned_time_arrival\"] = merged_df.loc[merged_df.index[::57], \"planned_time_departure\"]\n",
    "merged_df.loc[merged_df.index[::57], \"line_number_arrival\"] = merged_df.loc[merged_df.index[::57], \"line_number_departure\"]\n",
    "\n",
    "\n",
    "merged_df.loc[merged_df.index[56::57], \"line_number_departure\"] = 162\n",
    "merged_df.loc[merged_df.index[56::57], \"line_number_arrival\"] = 162\n",
    "\n",
    "index = merged_df.index[56::57]\n",
    "mask = merged_df.loc[index, \"real_time_departure\"] == None\n",
    "merged_df.loc[index, \"real_time_departure\"] = merged_df.loc[index, \"real_time_arrival\"]\n",
    "\n",
    "mask = merged_df.loc[index, \"planned_time_departure\"] == None\n",
    "merged_df.loc[index, \"planned_time_departure\"] = merged_df.loc[index, \"planned_time_arrival\"]\n",
    "\n",
    "mask = merged_df.loc[index, \"delay_departure\"] == None\n",
    "merged_df.loc[index, \"delay_departure\"] = merged_df.loc[index, \"delay_arrival\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45f9f9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57, 2)\n"
     ]
    }
   ],
   "source": [
    "print(filtered_stations.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e20d464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Station IDs: [ 0  0  0  0  0  0  0  0  0  0  0  0  6  6  6  6  6  6  6  6  6  6  6  6\n",
      "  6  6  6 10 10 10 10 10 10  6 10 10 10 10 10 10 10 10 10 10  9  9  9  9\n",
      "  9  9  9  9  9  9  9  9  9]\n",
      "(1558950, 1)\n"
     ]
    }
   ],
   "source": [
    "station_ids = filtered_stations['province_id'].values\n",
    "total_rows = 1558950\n",
    "repeated_ids = np.tile(station_ids, total_rows // len(station_ids) + 1)[:total_rows]\n",
    "\n",
    "repeated_ids_df = pd.DataFrame({'repeated_station_id': repeated_ids})\n",
    "\n",
    "print(\"Station IDs:\", station_ids)\n",
    "print(repeated_ids_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc3adc23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1558950, 16)\n"
     ]
    }
   ],
   "source": [
    "print(merged_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0d45f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1558950, 17)\n"
     ]
    }
   ],
   "source": [
    "merged_df = pd.concat([merged_df, repeated_ids_df], axis=1)\n",
    "print(merged_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798cebec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date  hour_of_day  temperature  dewpoint  relative_humidity  \\\n",
      "0  2014-01-01          0.0          2.6       2.2               97.0   \n",
      "1  2014-01-01          0.0          2.8      -0.3               91.0   \n",
      "2  2014-01-01          0.0          4.9       4.3               96.0   \n",
      "3  2014-01-01          0.0          5.5       4.6               94.0   \n",
      "4  2014-01-01          0.0          5.5       4.6               94.0   \n",
      "\n",
      "   precipitation  snowfall  wind_direction  wind_speed  pressure  province_id  \n",
      "0            0.0       0.0           210.0        10.8    1011.2            9  \n",
      "1            0.5       0.0           190.0        21.6    1011.3            8  \n",
      "2            0.0       0.0           220.0        18.0    1010.2           10  \n",
      "3            0.0       0.0           210.0        18.0    1009.2            7  \n",
      "4            0.0       0.0           210.0        21.6    1009.2            6  \n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    w.date,\n",
    "    EXTRACT(HOUR FROM w.hour) AS hour_of_day,\n",
    "    w.temperature AS temperature,\n",
    "    w.dewpoint AS dewpoint,\n",
    "    w.relative_humidity AS relative_humidity,\n",
    "    w.precipitation AS precipitation,\n",
    "    w.snowfall AS snowfall,\n",
    "    w.wind_direction AS wind_direction,\n",
    "    w.wind_speed AS wind_speed,\n",
    "    w.pressure AS pressure,\n",
    "    w.province AS province_id\n",
    "FROM WEATHER w\n",
    "GROUP BY \n",
    "    w.date,\n",
    "    EXTRACT(HOUR FROM w.hour),\n",
    "    w.temperature,\n",
    "    w.dewpoint,\n",
    "    w.relative_humidity,\n",
    "    w.precipitation,\n",
    "    w.snowfall,\n",
    "    w.wind_direction,\n",
    "    w.wind_speed,\n",
    "    w.pressure,\n",
    "    w.province\n",
    "ORDER BY w.date, hour_of_day;\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "weather = pd.read_sql(sqla.text(query), connection)\n",
    "\n",
    "print(weather.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d4270d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['time'] = pd.to_datetime(merged_df['planned_time_arrival'], format='%H:%M:%S')\n",
    "merged_df['hour'] = merged_df['time'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b673f8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of         departure_date  train_number line_number_departure real_time_arrival  \\\n",
      "0           2017-06-11          2119                   0/1          19:35:25   \n",
      "1           2017-06-11          2119                   0/1          19:37:30   \n",
      "2           2017-06-11          2119                   0/1          19:38:48   \n",
      "3           2017-06-11          2119                   0/1          19:41:36   \n",
      "4           2017-06-11          2119                   36N          19:43:27   \n",
      "...                ...           ...                   ...               ...   \n",
      "1558945     2024-04-12          2114                   162          17:31:51   \n",
      "1558946     2024-04-12          2114                   162          17:37:11   \n",
      "1558947     2024-04-12          2114                   162          17:41:24   \n",
      "1558948     2024-04-12          2114                   162          17:42:11   \n",
      "1558949     2024-04-12          2114                   162          17:44:41   \n",
      "\n",
      "        real_time_departure planned_time_arrival planned_time_departure  \\\n",
      "0                  19:35:25             19:33:00               19:33:00   \n",
      "1                  19:37:30             19:35:00               19:35:00   \n",
      "2                  19:39:51             19:36:00               19:37:00   \n",
      "3                  19:41:36             19:39:00               19:39:00   \n",
      "4                  19:45:12             19:41:00               19:43:00   \n",
      "...                     ...                  ...                    ...   \n",
      "1558945            17:33:32             17:09:00               17:10:00   \n",
      "1558946            17:37:11             17:14:00               17:14:00   \n",
      "1558947            17:41:24             17:19:00               17:19:00   \n",
      "1558948            17:42:11             17:21:00               17:21:00   \n",
      "1558949            17:44:41             17:22:00               17:22:00   \n",
      "\n",
      "         delay_arrival  delay_departure line_number_arrival  ptcar_longitude  \\\n",
      "0                  0.0            145.0                 0/1         4.336052   \n",
      "1                150.0            150.0                 0/1         4.347843   \n",
      "2                168.0            171.0                 0/1         4.356758   \n",
      "3                156.0            156.0                 0/1         4.362220   \n",
      "4                147.0            132.0                 0/1         4.361252   \n",
      "...                ...              ...                 ...              ...   \n",
      "1558945         1371.0           1412.0                 162         5.539755   \n",
      "1558946         1391.0           1391.0                 162         5.632256   \n",
      "1558947         1344.0           1344.0                 162         5.767977   \n",
      "1558948         1271.0           1271.0                 162         5.785475   \n",
      "1558949         1361.0           1361.0                 162         5.810610   \n",
      "\n",
      "         ptcar_latitude        date  holiday  weekend  day_after_rest  \\\n",
      "0             50.835801  2017-06-11        0     True           False   \n",
      "1             50.841176  2017-06-11        0     True           False   \n",
      "2             50.845311  2017-06-11        0     True           False   \n",
      "3             50.851780  2017-06-11        0     True           False   \n",
      "4             50.859600  2017-06-11        0     True           False   \n",
      "...                 ...         ...      ...      ...             ...   \n",
      "1558945       49.727420  2024-04-12        1    False           False   \n",
      "1558946       49.718254  2024-04-12        1    False           False   \n",
      "1558947       49.690932  2024-04-12        1    False           False   \n",
      "1558948       49.688284  2024-04-12        1    False           False   \n",
      "1558949       49.679971  2024-04-12        1    False           False   \n",
      "\n",
      "         repeated_station_id                time  hour  \n",
      "0                          0 1900-01-01 19:33:00  19.0  \n",
      "1                          0 1900-01-01 19:35:00  19.0  \n",
      "2                          0 1900-01-01 19:36:00  19.0  \n",
      "3                          0 1900-01-01 19:39:00  19.0  \n",
      "4                          0 1900-01-01 19:41:00  19.0  \n",
      "...                      ...                 ...   ...  \n",
      "1558945                    9 1900-01-01 17:09:00  17.0  \n",
      "1558946                    9 1900-01-01 17:14:00  17.0  \n",
      "1558947                    9 1900-01-01 17:19:00  17.0  \n",
      "1558948                    9 1900-01-01 17:21:00  17.0  \n",
      "1558949                    9 1900-01-01 17:22:00  17.0  \n",
      "\n",
      "[1558950 rows x 19 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(merged_df.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e34dbc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.merge(merged_df, weather, left_on=['departure_date', 'hour','repeated_station_id'], right_on=['date', 'hour_of_day','province_id'], how='left')\n",
    "final_df = final_df.drop(columns=['date_x', 'date_y', 'hour_of_day','time','hour'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a0d30d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  departure_date  train_number line_number_departure real_time_arrival  \\\n",
      "0     2017-06-11          2119                   0/1          19:35:25   \n",
      "1     2017-06-11          2119                   0/1          19:37:30   \n",
      "2     2017-06-11          2119                   0/1          19:38:48   \n",
      "3     2017-06-11          2119                   0/1          19:41:36   \n",
      "4     2017-06-11          2119                   36N          19:43:27   \n",
      "\n",
      "  real_time_departure planned_time_arrival planned_time_departure  \\\n",
      "0            19:35:25             19:33:00               19:33:00   \n",
      "1            19:37:30             19:35:00               19:35:00   \n",
      "2            19:39:51             19:36:00               19:37:00   \n",
      "3            19:41:36             19:39:00               19:39:00   \n",
      "4            19:45:12             19:41:00               19:43:00   \n",
      "\n",
      "   delay_arrival  delay_departure line_number_arrival  ...  \\\n",
      "0            0.0            145.0                 0/1  ...   \n",
      "1          150.0            150.0                 0/1  ...   \n",
      "2          168.0            171.0                 0/1  ...   \n",
      "3          156.0            156.0                 0/1  ...   \n",
      "4          147.0            132.0                 0/1  ...   \n",
      "\n",
      "   repeated_station_id  temperature  dewpoint  relative_humidity  \\\n",
      "0                    0         22.5      15.1               63.0   \n",
      "1                    0         22.5      15.1               63.0   \n",
      "2                    0         22.5      15.1               63.0   \n",
      "3                    0         22.5      15.1               63.0   \n",
      "4                    0         22.5      15.1               63.0   \n",
      "\n",
      "   precipitation  snowfall  wind_direction  wind_speed  pressure  province_id  \n",
      "0            0.0       0.0           290.0        18.0    1013.1          0.0  \n",
      "1            0.0       0.0           290.0        18.0    1013.1          0.0  \n",
      "2            0.0       0.0           290.0        18.0    1013.1          0.0  \n",
      "3            0.0       0.0           290.0        18.0    1013.1          0.0  \n",
      "4            0.0       0.0           290.0        18.0    1013.1          0.0  \n",
      "\n",
      "[5 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "print(final_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6560539f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to PostgreSQL successfully!\n",
      "Road: 57\n",
      "Skipping trajet starting at index 18069: contains NaN\n",
      "Skipping trajet starting at index 18240: contains NaN\n",
      "Skipping trajet starting at index 18468: contains NaN\n",
      "Skipping trajet starting at index 18525: contains NaN\n",
      "Skipping trajet starting at index 18753: contains NaN\n",
      "Skipping trajet starting at index 18810: contains NaN\n",
      "Skipping trajet starting at index 18924: contains NaN\n",
      "Skipping trajet starting at index 19038: contains NaN\n",
      "Skipping trajet starting at index 19095: contains NaN\n",
      "Skipping trajet starting at index 45771: contains NaN\n",
      "Skipping trajet starting at index 45942: contains NaN\n",
      "Skipping trajet starting at index 46113: contains NaN\n",
      "Skipping trajet starting at index 46284: contains NaN\n",
      "Skipping trajet starting at index 63099: contains NaN\n",
      "Skipping trajet starting at index 63783: contains NaN\n",
      "Skipping trajet starting at index 66462: contains NaN\n",
      "Skipping trajet starting at index 66804: contains NaN\n",
      "Skipping trajet starting at index 66918: contains NaN\n",
      "Skipping trajet starting at index 67260: contains NaN\n",
      "Skipping trajet starting at index 67431: contains NaN\n",
      "Skipping trajet starting at index 67830: contains NaN\n",
      "Skipping trajet starting at index 73188: contains NaN\n",
      "Skipping trajet starting at index 73701: contains NaN\n",
      "Skipping trajet starting at index 73758: contains NaN\n",
      "Skipping trajet starting at index 99180: contains NaN\n",
      "Skipping trajet starting at index 99237: contains NaN\n",
      "Skipping trajet starting at index 99522: contains NaN\n",
      "Skipping trajet starting at index 99636: contains NaN\n",
      "Skipping trajet starting at index 136116: contains NaN\n",
      "Skipping trajet starting at index 136230: contains NaN\n",
      "Skipping trajet starting at index 136287: contains NaN\n",
      "Skipping trajet starting at index 136800: contains NaN\n",
      "Skipping trajet starting at index 153843: contains NaN\n",
      "Skipping trajet starting at index 154071: contains NaN\n",
      "Skipping trajet starting at index 154242: contains NaN\n",
      "Skipping trajet starting at index 154299: contains NaN\n",
      "Skipping trajet starting at index 161880: contains NaN\n",
      "Skipping trajet starting at index 161937: contains NaN\n",
      "Skipping trajet starting at index 162051: contains NaN\n",
      "Skipping trajet starting at index 162336: contains NaN\n",
      "Skipping trajet starting at index 162393: contains NaN\n",
      "Skipping trajet starting at index 176814: contains NaN\n",
      "Skipping trajet starting at index 176985: contains NaN\n",
      "Skipping trajet starting at index 181716: contains NaN\n",
      "Skipping trajet starting at index 181830: contains NaN\n",
      "Skipping trajet starting at index 185991: contains NaN\n",
      "Skipping trajet starting at index 186447: contains NaN\n",
      "Skipping trajet starting at index 204231: contains NaN\n",
      "Skipping trajet starting at index 204516: contains NaN\n",
      "Skipping trajet starting at index 208734: contains NaN\n",
      "Skipping trajet starting at index 250458: contains NaN\n",
      "Skipping trajet starting at index 250743: contains NaN\n",
      "Skipping trajet starting at index 250800: contains NaN\n",
      "Skipping trajet starting at index 254619: contains NaN\n",
      "Skipping trajet starting at index 254676: contains NaN\n",
      "Skipping trajet starting at index 258267: contains NaN\n",
      "Skipping trajet starting at index 259293: contains NaN\n",
      "Skipping trajet starting at index 259521: contains NaN\n",
      "Skipping trajet starting at index 284487: contains NaN\n",
      "Skipping trajet starting at index 285057: contains NaN\n",
      "Skipping trajet starting at index 294804: contains NaN\n",
      "Skipping trajet starting at index 295602: contains NaN\n",
      "Skipping trajet starting at index 295773: contains NaN\n",
      "Skipping trajet starting at index 303981: contains NaN\n",
      "Skipping trajet starting at index 318858: contains NaN\n",
      "Skipping trajet starting at index 1427622: contains NaN\n",
      "Skipping trajet starting at index 1427679: contains NaN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sqlalchemy as sqla\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DB_USER = \"postgres\"\n",
    "DB_PASSWORD = \"a\"\n",
    "DB_HOST = \"localhost\"\n",
    "DB_PORT = \"5432\"\n",
    "DB_NAME = \"postgres\"\n",
    "\n",
    "connection_string = f\"postgresql+psycopg2://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "\n",
    "# Créer un moteur SQLAlchemy\n",
    "engine = sqla.create_engine(connection_string)\n",
    "\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        print(\"Connected to PostgreSQL successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "connection = engine.connect()\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT * FROM stations;\n",
    "\"\"\"\n",
    "\n",
    "stations = pd.read_sql(sqla.text(query), connection)\n",
    "\n",
    "data_province = [\n",
    "    (0,\"Bruxelles\", 4.367414882050033, 50.83642580516115),\n",
    "    (1,\"Antwerpen\", 4.721043779498864, 51.231900712244745),\n",
    "    (2,\"Limburg\", 5.4357209224443475, 50.98831159244354),\n",
    "    (3,\"Oost-Vlaanderen\", 3.8188605810442007, 51.03629347952689),\n",
    "    (4,\"Vlaams Brabant\", 4.59072072950776, 50.87303943005528),\n",
    "    (5,\"West-Vlaanderen\", 3.0620446668608117, 51.01019740435232),\n",
    "    (6,\"Brabant Wallon\", 4.589736973914637, 50.66605965845173),\n",
    "    (7,\"Hainaut\", 3.9681364929631004, 50.46387637972707),\n",
    "    (8,\"Liège\", 5.7372263688290746, 50.518876085615986),\n",
    "    (9,\"Luxembourg\", 5.516735682549194, 49.95849015160492),\n",
    "    (10,\"Namur\", 4.862346452029854, 50.252686235186815),\n",
    "]\n",
    "\n",
    "province = pd.DataFrame(data_province, columns=[\"province_id\", \"province_name\", \"province_long\", \"province_lat\"])\n",
    "\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    R = 6371  \n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    return R * c\n",
    "\n",
    "def find_nearest_province(station_row):\n",
    "    distances = haversine(\n",
    "        station_row['longitude'], station_row['latitude'],\n",
    "        province['province_long'].values, province['province_lat'].values\n",
    "    )\n",
    "    nearest_idx = distances.argmin()\n",
    "    return province.iloc[nearest_idx]\n",
    "\n",
    "# Apply the function\n",
    "nearest_provinces = stations.apply(find_nearest_province, axis=1)\n",
    "\n",
    "# Concatenate station info with nearest province\n",
    "stations_with_province = pd.concat([stations.reset_index(drop=True), nearest_provinces.reset_index(drop=True)], axis=1)\n",
    "\n",
    "road = [159,151,149,150,156,157,154,245,773,132,310,365,422,300,628,614,591,\n",
    " 540,118,178,235,281,480,108,642,625,623,557,558,384,199,559,645,123,195,73,\n",
    " 261,560,187,438,172,342,341,80,503,629,267,313,608,339,452,564,493,\n",
    " 317,704,757,70]\n",
    "\n",
    "filtered_stations = stations_with_province[stations_with_province['id'].isin(road)]\n",
    "filtered_stations = filtered_stations.drop(columns=['province_long', 'province_lat','region', 'latitude', 'longitude','geom', 'name', 'province_name'])\n",
    "filtered_stations['id'] = pd.Categorical(filtered_stations['id'], categories=road, ordered=True)\n",
    "filtered_stations = filtered_stations.sort_values('id').reset_index(drop=True)\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    td.id,\n",
    "    td.departure_date,\n",
    "    td.train_number,\n",
    "    td.relation,\n",
    "    td.train_service,\n",
    "    td.ptcar_number,\n",
    "    td.line_number_departure,\n",
    "    td.real_time_arrival,\n",
    "    td.real_time_departure,\n",
    "    td.planned_time_arrival,\n",
    "    td.planned_time_departure,\n",
    "    td.delay_arrival,\n",
    "    td.delay_departure,\n",
    "    ptcar.id AS ptcar_id,     \n",
    "    ptcar.name AS name_travel,    \n",
    "    td.line_number_arrival,\n",
    "    dep.name AS departure_station_name,\n",
    "    arr.name AS arrival_station_name,\n",
    "    ptcar.longitude AS ptcar_longitude,  \n",
    "    ptcar.latitude AS ptcar_latitude  \n",
    "FROM train_data td\n",
    "JOIN STATIONS ptcar ON td.ptcar_name = ptcar.id\n",
    "JOIN STATIONS dep ON td.station_departure = dep.id\n",
    "JOIN STATIONS arr ON td.station_arrival = arr.id\n",
    "WHERE td.station_arrival = '487'  \n",
    "  AND td.station_departure = '159'  \n",
    "ORDER BY td.id;\n",
    "\"\"\"\n",
    "\n",
    "line = pd.read_sql(sqla.text(query), connection)\n",
    "line =line.drop(columns=['train_service', 'relation'])\n",
    "print(\"Road:\", len(road))\n",
    "\n",
    "tempo = []\n",
    "clean = []\n",
    "road_index = 0\n",
    "\n",
    "for i, row in line.iterrows():\n",
    "    ptcar_id = row['ptcar_id']\n",
    "    if ptcar_id == road[road_index]:\n",
    "        tempo.append(row)\n",
    "        road_index += 1\n",
    "        if len(tempo) == len(road):\n",
    "            clean.extend(tempo)\n",
    "            tempo = []\n",
    "            road_index = 0\n",
    "    else:\n",
    "        tempo = []\n",
    "        road_index = 0\n",
    "\n",
    "clean_df = pd.DataFrame(clean)\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT * FROM TYPE_DAY;\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "type_day = pd.read_sql(sqla.text(query), connection)\n",
    "clean_df= clean_df.drop(columns=['departure_station_name','arrival_station_name','id', 'name_travel', 'ptcar_number','ptcar_id'])\n",
    "merged_df = pd.merge(clean_df, type_day, left_on='departure_date', right_on='date', how='left')\n",
    "merged_df.loc[merged_df.index[::57], \"delay_arrival\"] = 0\n",
    "\n",
    "merged_df.loc[merged_df.index[::57], \"real_time_arrival\"] = merged_df.loc[merged_df.index[::57], \"real_time_departure\"]\n",
    "merged_df.loc[merged_df.index[::57], \"planned_time_arrival\"] = merged_df.loc[merged_df.index[::57], \"planned_time_departure\"]\n",
    "merged_df.loc[merged_df.index[::57], \"line_number_arrival\"] = merged_df.loc[merged_df.index[::57], \"line_number_departure\"]\n",
    "\n",
    "\n",
    "merged_df.loc[merged_df.index[56::57], \"line_number_departure\"] = 162\n",
    "merged_df.loc[merged_df.index[56::57], \"line_number_arrival\"] = 162\n",
    "\n",
    "index = merged_df.index[56::57]\n",
    "mask = merged_df.loc[index, \"real_time_departure\"] == None\n",
    "merged_df.loc[index, \"real_time_departure\"] = merged_df.loc[index, \"real_time_arrival\"]\n",
    "\n",
    "mask = merged_df.loc[index, \"planned_time_departure\"] == None\n",
    "merged_df.loc[index, \"planned_time_departure\"] = merged_df.loc[index, \"planned_time_arrival\"]\n",
    "\n",
    "mask = merged_df.loc[index, \"delay_departure\"] == None\n",
    "merged_df.loc[index, \"delay_departure\"] = merged_df.loc[index, \"delay_arrival\"]\n",
    "\n",
    "station_ids = filtered_stations['province_id'].values\n",
    "total_rows = 1558950\n",
    "repeated_ids = np.tile(station_ids, total_rows // len(station_ids) + 1)[:total_rows]\n",
    "\n",
    "repeated_ids_df = pd.DataFrame({'repeated_station_id': repeated_ids})\n",
    "merged_df = pd.concat([merged_df, repeated_ids_df], axis=1)\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    w.date,\n",
    "    EXTRACT(HOUR FROM w.hour) AS hour_of_day,\n",
    "    w.temperature AS temperature,\n",
    "    w.dewpoint AS dewpoint,\n",
    "    w.relative_humidity AS relative_humidity,\n",
    "    w.precipitation AS precipitation,\n",
    "    w.snowfall AS snowfall,\n",
    "    w.wind_direction AS wind_direction,\n",
    "    w.wind_speed AS wind_speed,\n",
    "    w.pressure AS pressure,\n",
    "    w.province AS province_id\n",
    "FROM WEATHER w\n",
    "GROUP BY \n",
    "    w.date,\n",
    "    EXTRACT(HOUR FROM w.hour),\n",
    "    w.temperature,\n",
    "    w.dewpoint,\n",
    "    w.relative_humidity,\n",
    "    w.precipitation,\n",
    "    w.snowfall,\n",
    "    w.wind_direction,\n",
    "    w.wind_speed,\n",
    "    w.pressure,\n",
    "    w.province\n",
    "ORDER BY w.date, hour_of_day;\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "weather = pd.read_sql(sqla.text(query), connection)\n",
    "\n",
    "merged_df['time'] = pd.to_datetime(merged_df['planned_time_arrival'], format='%H:%M:%S')\n",
    "merged_df['hour'] = merged_df['time'].dt.hour\n",
    "\n",
    "final_df = pd.merge(merged_df, weather, left_on=['departure_date', 'hour','repeated_station_id'], right_on=['date', 'hour_of_day','province_id'], how='left')\n",
    "final_df = final_df.drop(columns=['date_x', 'date_y', 'hour_of_day','time','hour','repeated_station_id'])\n",
    "print(\"Middle of processing\")\n",
    "def parse_time_to_seconds(val):\n",
    "    if pd.isna(val):\n",
    "        return 0\n",
    "    try:\n",
    "        if isinstance(val, (int, float)):\n",
    "            return int(val)\n",
    "        if str(val).isdigit():\n",
    "            return int(val)\n",
    "        t = pd.to_datetime(val, format='%H:%M:%S', errors='coerce')\n",
    "        if pd.isna(t):\n",
    "            return 0\n",
    "        return t.hour * 3600 + t.minute * 60 + t.second\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "\n",
    "final_df['departure_date'] = pd.to_datetime(final_df['departure_date'], format='%Y-%m-%d')\n",
    "\n",
    "final_df['departure_year'] = final_df['departure_date'].dt.year\n",
    "final_df['departure_month'] = final_df['departure_date'].dt.month\n",
    "final_df['departure_day'] = final_df['departure_date'].dt.day\n",
    "final_df['departure_weekday'] = final_df['departure_date'].dt.weekday\n",
    "\n",
    "final_df['real_time_arrival'] = final_df['real_time_arrival'].apply(parse_time_to_seconds)\n",
    "final_df['planned_time_arrival'] = final_df['planned_time_arrival'].apply(parse_time_to_seconds)\n",
    "final_df['planned_time_departure'] = final_df['planned_time_departure'].apply(parse_time_to_seconds)\n",
    "final_df['real_time_departure'] = final_df['real_time_departure'].apply(parse_time_to_seconds)\n",
    "\n",
    "final_df = final_df.drop(columns=['departure_date', 'line_number_departure', 'line_number_arrival' ,'train_number'])\n",
    "final_df['delay_arrival'] = final_df['delay_arrival'].apply(parse_time_to_seconds)\n",
    "\n",
    "trajet_valid = []\n",
    "\n",
    "for i in range(0, len(final_df), 57):\n",
    "    trajet = final_df.iloc[i:i+57]\n",
    "    if trajet.shape[0] < 57:\n",
    "        print(\"Not enough data for this trajet, skipping\")\n",
    "        continue\n",
    "\n",
    "    if trajet.isnull().values.any():\n",
    "        continue\n",
    "\n",
    "    if trajet['delay_arrival'].abs().max() > 36000:\n",
    "        print(\"Delay arrival exceeds 10000 seconds, skipping\")\n",
    "        continue\n",
    "\n",
    "    trajet_valid.append(trajet)\n",
    "\n",
    "trajet_valid_df = pd.concat(trajet_valid, ignore_index=True)\n",
    "trajet_valid_df[\"stop_index\"] = trajet_valid_df.index % 57\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f276b6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1555131, 25)\n"
     ]
    }
   ],
   "source": [
    "print(trajet_valid_df.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
