{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b1312b8",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae6ae8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sqlalchemy as sqla\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DB_USER = \"postgres\"\n",
    "DB_PASSWORD = \"a\"\n",
    "DB_HOST = \"localhost\"\n",
    "DB_PORT = \"5432\"\n",
    "DB_NAME = \"postgres\"\n",
    "\n",
    "connection_string = f\"postgresql+psycopg2://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "\n",
    "# Cr√©er un moteur SQLAlchemy\n",
    "engine = sqla.create_engine(connection_string)\n",
    "\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        print(\"Connected to PostgreSQL successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "connection = engine.connect()\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    td.id,\n",
    "    td.departure_date,\n",
    "    td.train_number,\n",
    "    td.relation,\n",
    "    td.train_service,\n",
    "    td.ptcar_number,\n",
    "    td.line_number_departure,\n",
    "    td.real_time_arrival,\n",
    "    td.real_time_departure,\n",
    "    td.planned_time_arrival,\n",
    "    td.planned_time_departure,\n",
    "    td.delay_arrival,\n",
    "    td.delay_departure,\n",
    "    ptcar.id AS ptcar_id,     \n",
    "    ptcar.name AS name_travel,    \n",
    "    td.line_number_arrival,\n",
    "    dep.name AS departure_station_name,\n",
    "    arr.name AS arrival_station_name,\n",
    "    ptcar.longitude AS ptcar_longitude,  \n",
    "    ptcar.latitude AS ptcar_latitude  \n",
    "FROM train_data td\n",
    "JOIN STATIONS ptcar ON td.ptcar_name = ptcar.id\n",
    "JOIN STATIONS dep ON td.station_departure = dep.id\n",
    "JOIN STATIONS arr ON td.station_arrival = arr.id\n",
    "WHERE td.station_arrival = '487'  \n",
    "  AND td.station_departure = '159'  \n",
    "ORDER BY td.id;\n",
    "\"\"\"\n",
    "\n",
    "line = pd.read_sql(sqla.text(query), connection)\n",
    "line =line.drop(columns=['train_service', 'relation'])\n",
    "\n",
    "road = [159,151,149,150,156,157,154,245,773,132,310,365,422,300,628,614,591,\n",
    " 540,118,178,235,281,480,108,642,625,623,557,558,384,199,559,645,123,195,73,\n",
    " 261,560,187,438,172,342,341,80,503,629,267,313,608,339,452,564,493,\n",
    " 317,704,757,70]\n",
    "\n",
    "print(\"Road:\", len(road))\n",
    "\n",
    "tempo = []\n",
    "clean = []\n",
    "road_index = 0\n",
    "\n",
    "for i, row in line.iterrows():\n",
    "    ptcar_id = row['ptcar_id']\n",
    "    if ptcar_id == road[road_index]:\n",
    "        tempo.append(row)\n",
    "        road_index += 1\n",
    "        if len(tempo) == len(road):\n",
    "            clean.extend(tempo)\n",
    "            tempo = []\n",
    "            road_index = 0\n",
    "    else:\n",
    "        tempo = []\n",
    "        road_index = 0\n",
    "\n",
    "clean_df = pd.DataFrame(clean)\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT * FROM TYPE_DAY;\n",
    "\"\"\"\n",
    "\n",
    "type_day = pd.read_sql(sqla.text(query), connection)\n",
    "clean_df= clean_df.drop(columns=['departure_station_name','arrival_station_name','id', 'name_travel', 'ptcar_number','ptcar_id'])\n",
    "merged_df = pd.merge(clean_df, type_day, left_on='departure_date', right_on='date', how='left')\n",
    "merged_df.loc[merged_df.index[::57], \"delay_arrival\"] = 0\n",
    "\n",
    "merged_df.loc[merged_df.index[::57], \"real_time_arrival\"] = merged_df.loc[merged_df.index[::57], \"real_time_departure\"]\n",
    "merged_df.loc[merged_df.index[::57], \"planned_time_arrival\"] = merged_df.loc[merged_df.index[::57], \"planned_time_departure\"]\n",
    "merged_df.loc[merged_df.index[::57], \"line_number_arrival\"] = merged_df.loc[merged_df.index[::57], \"line_number_departure\"]\n",
    "\n",
    "\n",
    "merged_df.loc[merged_df.index[56::57], \"line_number_departure\"] = 162\n",
    "merged_df.loc[merged_df.index[56::57], \"line_number_arrival\"] = 162\n",
    "\n",
    "index = merged_df.index[56::57]\n",
    "mask = merged_df.loc[index, \"real_time_departure\"] == None\n",
    "merged_df.loc[index, \"real_time_departure\"] = merged_df.loc[index, \"real_time_arrival\"]\n",
    "\n",
    "mask = merged_df.loc[index, \"planned_time_departure\"] == None\n",
    "merged_df.loc[index, \"planned_time_departure\"] = merged_df.loc[index, \"planned_time_arrival\"]\n",
    "\n",
    "mask = merged_df.loc[index, \"delay_departure\"] == None\n",
    "merged_df.loc[index, \"delay_departure\"] = merged_df.loc[index, \"delay_arrival\"]\n",
    "#merged_df = merged_df.ffill(axis=1).bfill(axis=1)\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "w.date,\n",
    "EXTRACT(HOUR FROM w.hour) AS hour_of_day,\n",
    "AVG(w.temperature) AS avg_temperature,\n",
    "AVG(w.dewpoint) AS avg_dewpoint,\n",
    "AVG(w.relative_humidity) AS avg_relative_humidity,\n",
    "AVG(w.precipitation) AS avg_precipitation,\n",
    "AVG(w.snowfall) AS avg_snowfall,\n",
    "AVG(w.wind_direction) AS avg_wind_direction,\n",
    "AVG(w.wind_speed) AS avg_wind_speed,\n",
    "AVG(w.pressure) AS avg_pressure\n",
    "FROM WEATHER w\n",
    "GROUP BY w.date, hour_of_day\n",
    "ORDER BY w.date, hour_of_day;\n",
    "\"\"\"\n",
    "\n",
    "weather = pd.read_sql(sqla.text(query), connection)\n",
    "\n",
    "print(\"In Loading...\")\n",
    "\n",
    "merged_df['time'] = pd.to_datetime(merged_df['planned_time_arrival'], format='%H:%M:%S')\n",
    "merged_df['hour'] = merged_df['time'].dt.hour\n",
    "final_df = pd.merge(merged_df, weather, left_on=['departure_date', 'hour'], right_on=['date', 'hour_of_day'], how='left')\n",
    "final_df = final_df.drop(columns=['date_x', 'date_y', 'hour_of_day','time','hour'])\n",
    "\n",
    "def parse_time_to_seconds(val):\n",
    "    if pd.isna(val):\n",
    "        return 0\n",
    "    try:\n",
    "        if isinstance(val, (int, float)):\n",
    "            return int(val)\n",
    "        if str(val).isdigit():\n",
    "            return int(val)\n",
    "        t = pd.to_datetime(val, format='%H:%M:%S', errors='coerce')\n",
    "        if pd.isna(t):\n",
    "            return 0\n",
    "        return t.hour * 3600 + t.minute * 60 + t.second\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "\n",
    "final_df['departure_date'] = pd.to_datetime(final_df['departure_date'], format='%Y-%m-%d')\n",
    "\n",
    "final_df['departure_year'] = final_df['departure_date'].dt.year\n",
    "final_df['departure_month'] = final_df['departure_date'].dt.month\n",
    "final_df['departure_day'] = final_df['departure_date'].dt.day\n",
    "final_df['departure_weekday'] = final_df['departure_date'].dt.weekday\n",
    "\n",
    "final_df['real_time_arrival'] = final_df['real_time_arrival'].apply(parse_time_to_seconds)\n",
    "final_df['planned_time_arrival'] = final_df['planned_time_arrival'].apply(parse_time_to_seconds)\n",
    "final_df['planned_time_departure'] = final_df['planned_time_departure'].apply(parse_time_to_seconds)\n",
    "final_df['real_time_departure'] = final_df['real_time_departure'].apply(parse_time_to_seconds)\n",
    "\n",
    "final_df = final_df.drop(columns=['departure_date', 'line_number_departure', 'line_number_arrival' ,'train_number'])\n",
    "final_df['delay_arrival'] = final_df['delay_arrival'].apply(parse_time_to_seconds)\n",
    "\n",
    "trajet_valid = []\n",
    "\n",
    "for i in range(0, len(final_df), 57):\n",
    "    trajet = final_df.iloc[i:i+57]\n",
    "    if trajet.shape[0] < 57:\n",
    "        print(\"Not enough data for this trajet, skipping\")\n",
    "        continue\n",
    "\n",
    "    if trajet.isnull().values.any():\n",
    "        print(f\"Skipping trajet starting at index {i}: contains NaN\")\n",
    "        continue\n",
    "\n",
    "    if trajet['delay_arrival'].abs().max() > 36000:\n",
    "        print(\"Delay arrival exceeds 10000 seconds, skipping\")\n",
    "        continue\n",
    "\n",
    "    trajet_valid.append(trajet)\n",
    "\n",
    "trajet_valid_df = pd.concat(trajet_valid, ignore_index=True)\n",
    "trajet_valid_df[\"stop_index\"] = trajet_valid_df.index % 57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7f83e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "stop = 57\n",
    "features = 23\n",
    "travels = trajet_valid_df.shape[0] // stop\n",
    "print(\"Travels:\", travels)\n",
    "print(\"Features:\", features)\n",
    "print(\"Stop:\", stop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efd6b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = final_df.values.reshape(travels,stop,features)\n",
    "print(data.shape)\n",
    "indices = np.arange(data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54489a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(indices))\n",
    "test_size = len(indices) - train_size\n",
    "\n",
    "train_data = data[:train_size]\n",
    "test_data = data[train_size:]\n",
    "\n",
    "print(\"Train shape:\", train_data.shape)\n",
    "print(\"Test shape:\", test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415f36c2",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da343ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Travels(Dataset):\n",
    "    def __init__(self,trips,input_length, target, target_index = 4):\n",
    "        self.trips = trips\n",
    "        self.input_length = input_length\n",
    "        self.target = target\n",
    "        self.target_index = target_index\n",
    "\n",
    "        all_idx = list(range(self.trips.shape[2]))\n",
    "        exclude_idx = [target_index]\n",
    "        self.continuous_features_idx = [i for i in all_idx if i not in exclude_idx]\n",
    "    def __len__(self):\n",
    "        return self.trips.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        trip = self.trips[idx]\n",
    "        X_cont = trip[:self.input_length, :].astype(np.float32)\n",
    "        \n",
    "        y = trip[self.target, self.target_index]\n",
    "\n",
    "        return (\n",
    "            torch.tensor(X_cont, dtype=torch.float32),                     \n",
    "            torch.tensor(y, dtype=torch.float32)\n",
    "        ) \n",
    "\n",
    "class ModelRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(ModelRNN, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "\n",
    "        self.rnn = nn.RNN(input_size=self.input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x_cont):\n",
    "\n",
    "        out,h_n  = self.rnn(x_cont)\n",
    "        h_last = h_n[-1]\n",
    "        return self.fc(h_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7030f07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "input_length = 1\n",
    "target = 1\n",
    "\n",
    "trained = Travels(train_data, input_length, target)\n",
    "test = Travels(test_data, input_length, target)\n",
    "\n",
    "model = ModelRNN(input_size= features, hidden_size=  features*2, num_layers=2, output_size=1)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#model = model.to(device)\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 3\n",
    "\n",
    "train_loader = DataLoader(trained, batch_size=4, shuffle=True)\n",
    "test_loader = DataLoader(test, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0a62af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs,model):\n",
    "    print(\"Training...\", num_epochs, \"epochs\", \"model:\", model)\n",
    "    train_avg_loss = []\n",
    "    test_avg_loss = []\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = []\n",
    "        model.train()\n",
    "        for X_cont, y in train_loader:\n",
    "            #X_cont = X_cont.to(device)\n",
    "            #y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_cont)\n",
    "\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.append(loss.item())\n",
    "        train_avg_loss.append(sum(train_loss) / len(train_loss))\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_avg_loss[-1]:.4f}\")\n",
    "        model.eval()\n",
    "        test_losses = []\n",
    "        with torch.no_grad():\n",
    "            for X_cont, y in test_loader:\n",
    "                #X_cont = X_cont.to(device)\n",
    "                #y = y.to(device)\n",
    "                outputs = model(X_cont)\n",
    "                loss = criterion(outputs, y)\n",
    "                test_losses.append(loss.item())\n",
    "        test_avg_loss.append(sum(test_losses) / len(test_losses))\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Test Loss: {test_avg_loss[-1]:.4f}\")\n",
    "\n",
    "    return train_avg_loss, test_avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a086002e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_avg_loss, test_avg_loss = train(num_epochs,model)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
